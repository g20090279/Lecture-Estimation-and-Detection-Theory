- Key Words: Bayesian Linear Model
- Last revised: Apr. 4, 2023

---

# Review of Multivariate Guassian Distribution

# Bayesian Linear Model

As the linear model for classical estimation where the parameter is deterministic, i.e. $\boldsymbol{x}=\boldsymbol{1}A+\boldsymbol{w}$, where $\boldsymbol{x}$ is the observation vector, $\boldsymbol{w}$ is the noise vector, and $A$ is the parameter, there is also a Bayesian equivalent of the general linear model

$$\boldsymbol{x}=H\boldsymbol{\theta}+\boldsymbol{w},$$

where $\boldsymbol{x}\in\mathbb{C}^{N\times1}$, $H\in\mathbb{C}^{N\times P}$ and $\boldsymbol{\theta}\in\mathbb{C}^{p\times1}$. The prior PDF of $\boldsymbol{x}$ and $\boldsymbol{w}$ are $\mathcal{CN}(\boldsymbol{\mu_{\theta}},C_{\boldsymbol{\theta}})$ and $\mathcal{CN}(\boldsymbol{0},C_{\boldsymbol{w}})$, respectively.

## Why Is Bayesian Linear Model Useful?

To make this linear model useful, we want to have an explicit expression for the posterior PDF $p(\boldsymbol{\theta|x})$, by which we may have the possibility to obtain the expectation of the posterior PDF (which is the MMSE estimator). If $p(\boldsymbol{\theta|x})$ is Gaussian, it is more likely to have a closed form of the estimator. Form the properties of the multivariate Guassian, we know that if $\boldsymbol{x}$ and $\boldsymbol{\theta}$ are jointly Gaussian, then the posterior PDF is also Gaussian.

Indeed, 